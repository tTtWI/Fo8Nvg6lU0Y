{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Layer Weights\n",
    "\n",
    "This ipython notebook can be used to generate cluster of CNN layer weights from a set of images, and cluster those images based on the layer weights. Used to generate a sample image cluster for the image clustering UI. \n",
    "\n",
    "## Requirements\n",
    "* Environment variables for CAFFE_HOME set to the location of the CAFFE installation for NVIDIA's fork of the caffe framework\n",
    "* Environment variable for DIGITS_HOME set to the location of the NVIDIA Digits installation\n",
    "* Existing model for doing the feature detection\n",
    "* sklearn version > 0.18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# get the location of your caffe and digits\n",
    "caffe_home = os.environ['CAFFE_HOME']\n",
    "digits_home = os.environ['DIGITS_HOME']\n",
    "\n",
    "# set the model directory and the mean file\n",
    "model_jobid = \"20150910-211807-d169\"\n",
    "model_dir = \"/data/digits/jobs/%s/\" % model_jobid\n",
    "mean_file = \"/data/digits/jobs/20150910-150610-9373/mean.binaryproto\"\n",
    "test_image_file = r\"/data/1020/data/military uniform/937.jpg\"\n",
    "\n",
    "# import some digits webserver code\n",
    "import sys\n",
    "sys.path.append(caffe_home +\"/python/\")\n",
    "sys.path.append(digits_home)\n",
    "sys.path.append(digits_home + 'digits')\n",
    "\n",
    "import digits\n",
    "from digits import config\n",
    "config.load_config()\n",
    "import utils\n",
    "import shutil\n",
    "from job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from caffe.proto import caffe_pb2\n",
    "import sklearn \n",
    "import glob\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configure plotting\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the CNN object using the caffemodel file for model parameters \n",
    "# and the prototxt for defining the model architecture\n",
    "netdeploymet_proto = model_dir + \"deploy.prototxt\"\n",
    "latest_model_iteration = model_dir + \"snapshot_iter_505860.caffemodel\"\n",
    "\n",
    "net = caffe.Net(netdeploymet_proto,\n",
    "               latest_model_iteration,\n",
    "               caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#im = caffe.io.load_image(\"/home/sander/Documents/Pixelate/corgi_test.png\")\n",
    "im = caffe.io.load_image(test_image_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilize code from the DIGITS web interface for its image handling \n",
    "# and consistency with the DIGITS interface\n",
    "def getlayerweightsWeb(imfname, net=net,\\\n",
    "                  deploy_file=netdeploymet_proto,\n",
    "                  meanfname=mean_file,\n",
    "                  jobid=model_jobid\n",
    "                 layer='fc6'):\n",
    "    \n",
    "    \n",
    "    network = caffe_pb2.NetParameter()\n",
    "    infile = open(deploy_file)\n",
    "    text_format.Merge(infile.read(), network)\n",
    "\n",
    "    imtest = imfname\n",
    "    image = utils.image.load_image(imtest)\n",
    "    # need programmatic access\n",
    "    dataset = Job.load(jobid)\n",
    "    # get transformer\n",
    "    data_shape = None\n",
    "    channel_swap = None\n",
    "    mean_pixel = None\n",
    "    data_shape = network.input_dim[:4]\n",
    "\n",
    "    channel_swap = (2,1,0)\n",
    "    infile = open(meanfname,'rb')\n",
    "    blob = caffe_pb2.BlobProto()\n",
    "    blob.MergeFromString(infile.read())\n",
    "    mean_pixel = np.reshape(blob.data,\\\n",
    "                                (\\\n",
    "                                    dataset.image_dims[2],\\\n",
    "                                    dataset.image_dims[0],\\\n",
    "                                    dataset.image_dims[1],\\\n",
    "                                    )\\\n",
    "                                ).mean(1).mean(1)\n",
    "\n",
    "\n",
    "    t = caffe.io.Transformer(\n",
    "                    inputs = {'data':  data_shape}\n",
    "                    )\n",
    "    t.set_transpose('data', (2,0,1))\n",
    "    t.set_channel_swap('data', channel_swap)\n",
    "    t.set_mean('data', mean_pixel)\n",
    "    image = np.array(image)\n",
    "    preprocessed = None\n",
    "    try:\n",
    "        preprocessed = t.preprocess('data',image)\n",
    "    except RuntimeError as e:\n",
    "        print e\n",
    "        return None\n",
    "        \n",
    "    test_shape = (1,) + preprocessed.shape\n",
    "    if net.blobs['data'].data.shape != test_shape:\n",
    "                net.blobs['data'].reshape(*test_shape)\n",
    "\n",
    "    net.blobs['data'].data[...] = preprocessed\n",
    "    output = net.forward(end=layer)\n",
    "    \n",
    "    return output[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the directory names as labels \n",
    "# grab a sample of 100 image filenames from each directory\n",
    "\n",
    "catlist = glob.glob(\"/data_ebs/1020/data/*\")\n",
    "imcatlist = []\n",
    "samplesize = 100\n",
    "for cat in catlist:\n",
    "    imcatlist.extend(glob.glob(cat+\"/*\")[:samplesize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samplecategories = 10\n",
    "n_samples = samplecategories*samplesize\n",
    "imweights = np.zeros([n_samples,4096],dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the features from the 7th layer of the NN \n",
    "# and save the numpy array \n",
    "for i, im in enumerate(imcatlist[:n_samples]): \n",
    "    imweights[i,:] = getlayerweightsWeb(im)\n",
    "    \n",
    "np.save(\"imweights.npy\",imweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reload the weights if you've previously run the code\n",
    "if imweights == None or imweights == np.zeros([n_samples,4096],dtype=float): \n",
    "    imweights = np.load(\"imweights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the images into reduced 3d space using TSNE\n",
    "model = TSNE(n_components=3)\n",
    "trweights3d = model.fit_transform(imweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a map between the categories and the index for the files\n",
    "namemap = {j : i for i, j in enumerate(set([f.split(\"/\")[-2] for f in imfilelist]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy the images used for the clustering example \n",
    "for f in imcatlist[:n_samples]:\n",
    "    ext = f.split(\"/\")[-1].split(\".\")[-1]\n",
    "    fname = f.split(\"/\")[-1].split(\".\")[0]\n",
    "    subdir = f.split(\"/\")[-2]\n",
    "    shutil.copyfile(f, \"/data/staging/clustering_example/%s_%i.%s\" % (fname, namemap[subdir],ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the 3d coordinates for the image cluster\n",
    "outfile = open(\"/data/staging/clustering_example/imdata3d.txt\",\"w\")\n",
    "for i, f in enumerate(imfilelist[:1000]):\n",
    "    ext = f.split(\"/\")[-1].split(\".\")[-1]\n",
    "    fname = f.split(\"/\")[-1].split(\".\")[0]\n",
    "    subdir = f.split(\"/\")[-2]\n",
    "    newfile = \"%s_%i.%s\" % (fname, namemap[subdir],ext)\n",
    "    outfile.write(\"%s\\t%3.5f\\t%3.5f\\t%3.5f\\n\" % (newfile, trweights3d[i,0], trweights3d[i,1], trweights3d[i,2] ))\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
